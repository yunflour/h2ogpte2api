"""
H2OGPTE to OpenAI API è½¬æ¢æœåŠ¡

å°† H2OGPTE çš„ API å°è£…ä¸ºæ ‡å‡†çš„ OpenAI API æ ¼å¼
æ”¯æŒ /v1/models å’Œ /v1/chat/completions æ¥å£
"""
import json
import uuid
import time
from typing import Optional
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, Header
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse

from config import config
from models import (
    Model, ModelsResponse,
    ChatMessage, ChatCompletionRequest, ChatCompletionResponse,
    ChatCompletionChoice, ChatCompletionChunk, ChatCompletionChunkChoice,
    DeltaMessage, Usage
)
from h2ogpte_client import h2ogpte_client
from session_manager import SessionManager


# ============ åº”ç”¨åˆå§‹åŒ– ============

session_manager = SessionManager(h2ogpte_client)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    print(f"ğŸš€ H2OGPTE to OpenAI API æœåŠ¡å¯åŠ¨")
    print(f"ğŸ“¡ ç›‘å¬åœ°å€: http://{config.HOST}:{config.PORT}")
    print(f"ğŸ”— ç›®æ ‡æœåŠ¡: {config.H2OGPTE_BASE_URL}")
    print(f"ğŸ”„ å¯åŠ¨ä¼šè¯æ± ç®¡ç†å™¨...")
    await session_manager.start()
    yield
    print("ğŸ‘‹ æ­£åœ¨åœæ­¢ä¼šè¯æ± ...")
    await session_manager.stop()
    print("ğŸ‘‹ æœåŠ¡å…³é—­")


app = FastAPI(
    title="H2OGPTE to OpenAI API",
    description="å°† H2OGPTE API è½¬æ¢ä¸ºæ ‡å‡† OpenAI API æ ¼å¼",
    version="1.0.0",
    lifespan=lifespan
)

# CORS é…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ============ è¾…åŠ©å‡½æ•° ============

def generate_completion_id() -> str:
    """ç”Ÿæˆå”¯ä¸€çš„è¡¥å…¨ ID"""
    return f"chatcmpl-{uuid.uuid4().hex[:24]}"


def verify_api_key(authorization: Optional[str]) -> bool:
    """
    éªŒè¯ API Key
    
    æ”¯æŒæ ¼å¼:
    - Bearer sk-xxx
    - sk-xxx
    
    å¦‚æœæœªé…ç½® API_KEYï¼Œåˆ™è·³è¿‡éªŒè¯
    """
    # å¦‚æœæœªé…ç½® API_KEYï¼Œè·³è¿‡éªŒè¯
    if not config.API_KEY:
        return True
    
    if not authorization:
        return False
    
    # æå– token
    token = authorization
    if authorization.startswith("Bearer "):
        token = authorization[7:]
    
    # éªŒè¯ token
    return token == config.API_KEY


# ============ API ç«¯ç‚¹ ============

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "H2OGPTE to OpenAI API",
        "docs": "/docs",
        "endpoints": ["/v1/models", "/v1/chat/completions"]
    }


@app.get("/v1/models")
async def list_models(authorization: Optional[str] = Header(None)):
    """
    è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
    
    å…¼å®¹ OpenAI API: GET /v1/models
    """
    if not verify_api_key(authorization):
        raise HTTPException(status_code=401, detail="Invalid API Key")
    
    try:
        models_data = await h2ogpte_client.list_models()
        
        models = [
            Model(
                id=m.get("id", m.get("name", f"model-{i}")),
                created=int(time.time()),
                owned_by="h2ogpte"
            )
            for i, m in enumerate(models_data)
        ]
        
        return ModelsResponse(data=models)
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}")


@app.get("/v1/models/{model_id}")
async def get_model(model_id: str, authorization: Optional[str] = Header(None)):
    """
    è·å–å•ä¸ªæ¨¡å‹ä¿¡æ¯
    
    å…¼å®¹ OpenAI API: GET /v1/models/{model}
    """
    if not verify_api_key(authorization):
        raise HTTPException(status_code=401, detail="Invalid API Key")
    
    return Model(id=model_id, created=int(time.time()), owned_by="h2ogpte")


@app.post("/v1/chat/completions")
async def create_chat_completion(
    request: ChatCompletionRequest,
    authorization: Optional[str] = Header(None)
):
    """
    åˆ›å»ºèŠå¤©è¡¥å…¨
    
    å…¼å®¹ OpenAI API: POST /v1/chat/completions
    æ”¯æŒæµå¼å’Œéæµå¼å“åº”
    """
    if not verify_api_key(authorization):
        raise HTTPException(status_code=401, detail="Invalid API Key")
    
    try:
        # æå–ç³»ç»Ÿæç¤ºè¯å¹¶æ„å»ºå®Œæ•´çš„å¯¹è¯ä¸Šä¸‹æ–‡
        system_prompt = None
        conversation_parts = []
        
        for msg in request.messages:
            if msg.role == "system":
                system_prompt = msg.content
            elif msg.role == "user":
                conversation_parts.append(f"User: {msg.content}")
            elif msg.role == "assistant":
                conversation_parts.append(f"Assistant: {msg.content}")
        
        # å°†å¯¹è¯å†å²æ‹¼æ¥æˆå®Œæ•´çš„æ¶ˆæ¯
        # å¦‚æœåªæœ‰ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™æ‹¼æ¥å®Œæ•´ä¸Šä¸‹æ–‡
        if len(conversation_parts) == 1:
            user_message = request.messages[-1].content if request.messages else ""
        else:
            # æ‹¼æ¥å¯¹è¯å†å²ï¼Œæ·»åŠ æœ€ç»ˆçš„æç¤º
            user_message = "\n".join(conversation_parts)
            # å¦‚æœæœ€åä¸æ˜¯ç”¨æˆ·æ¶ˆæ¯ï¼Œæ·»åŠ æç¤º
            if not user_message.endswith(conversation_parts[-1]):
                user_message += "\nAssistant:"
        
        # å¦‚æœæ²¡æœ‰ç”¨æˆ·æ¶ˆæ¯ï¼Œä½¿ç”¨æœ€åä¸€æ¡æ¶ˆæ¯
        if not user_message and request.messages:
            user_message = request.messages[-1].content
        
        # ä»ä¼šè¯æ± è·å–èŠå¤©ä¼šè¯
        chat_id = await session_manager.get_session()
        
        if request.stream:
            # æµå¼å“åº”
            return StreamingResponse(
                stream_chat_completion(
                    chat_id=chat_id,
                    message=user_message,
                    model=request.model,
                    system_prompt=system_prompt,
                    temperature=request.temperature,
                    max_tokens=request.max_tokens
                ),
                media_type="text/event-stream"
            )
        else:
            # éæµå¼å“åº”
            response_content = await h2ogpte_client.send_message(
                message=user_message,
                chat_id=chat_id,
                model=request.model,
                system_prompt=system_prompt,
                temperature=request.temperature,
                max_tokens=request.max_tokens
            )
            
            # å›æ”¶èŠå¤©ä¼šè¯
            await session_manager.recycle_session(chat_id)
            
            completion_id = generate_completion_id()
            
            return ChatCompletionResponse(
                id=completion_id,
                model=request.model,
                choices=[
                    ChatCompletionChoice(
                        index=0,
                        message=ChatMessage(
                            role="assistant",
                            content=response_content
                        ),
                        finish_reason="stop"
                    )
                ],
                usage=Usage(
                    prompt_tokens=len(user_message) // 4,  # ç²—ç•¥ä¼°ç®—
                    completion_tokens=len(response_content) // 4,
                    total_tokens=(len(user_message) + len(response_content)) // 4
                )
            )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"èŠå¤©è¡¥å…¨å¤±è´¥: {str(e)}")


async def stream_chat_completion(
    chat_id: str,
    message: str,
    model: str,
    system_prompt: Optional[str],
    temperature: float,
    max_tokens: Optional[int]
):
    """ç”Ÿæˆæµå¼èŠå¤©è¡¥å…¨å“åº”"""
    completion_id = generate_completion_id()
    created = int(time.time())
    
    # å‘é€è§’è‰²ä¿¡æ¯
    chunk = ChatCompletionChunk(
        id=completion_id,
        created=created,
        model=model,
        choices=[
            ChatCompletionChunkChoice(
                index=0,
                delta=DeltaMessage(role="assistant"),
                finish_reason=None
            )
        ]
    )
    yield f"data: {chunk.model_dump_json()}\n\n"
    
    try:
        # æµå¼è·å–å†…å®¹
        async for content_chunk in h2ogpte_client.send_message_stream(
            message=message,
            chat_id=chat_id,
            model=model,
            system_prompt=system_prompt,
            temperature=temperature,
            max_tokens=max_tokens
        ):
            chunk = ChatCompletionChunk(
                id=completion_id,
                created=created,
                model=model,
                choices=[
                    ChatCompletionChunkChoice(
                        index=0,
                        delta=DeltaMessage(content=content_chunk),
                        finish_reason=None
                    )
                ]
            )
            yield f"data: {chunk.model_dump_json()}\n\n"
    
    except Exception as e:
        # å‘é€é”™è¯¯ä¿¡æ¯ä½œä¸ºå†…å®¹
        error_chunk = ChatCompletionChunk(
            id=completion_id,
            created=created,
            model=model,
            choices=[
                ChatCompletionChunkChoice(
                    index=0,
                    delta=DeltaMessage(content=f"[Error: {str(e)}]"),
                    finish_reason=None
                )
            ]
        )
        yield f"data: {error_chunk.model_dump_json()}\n\n"
    
    # å‘é€ç»“æŸä¿¡å·
    final_chunk = ChatCompletionChunk(
        id=completion_id,
        created=created,
        model=model,
        choices=[
            ChatCompletionChunkChoice(
                index=0,
                delta=DeltaMessage(),
                finish_reason="stop"
            )
        ]
    )
    yield f"data: {final_chunk.model_dump_json()}\n\n"
    yield "data: [DONE]\n\n"
    
    # å›æ”¶èŠå¤©ä¼šè¯
    await session_manager.recycle_session(chat_id)


# ============ å¯åŠ¨å…¥å£ ============

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app",
        host=config.HOST,
        port=config.PORT,
        reload=True
    )
